I) Partie Bash:
* Fichier : download.sh
  ==> Contient le script pour télécharger les pages html en locale en utilisant la commande "curl"
      ( Les fichiers html seront stockés dans le dossier "pages"
  ==> script de lancement du partie scrapy : "scrapy runspider sofifa.py"
* projet scrapy qui permet de crawler les pages html stockés dans le dossier "pages".
  ===> enregistrer les données dans le fichier data.csv
  
